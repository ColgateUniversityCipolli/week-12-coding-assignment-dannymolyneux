\documentclass{article}
\usepackage[margin=1.0in]{geometry} % To set margins
\usepackage{amsmath}  % This allows me to use the align functionality.
                      % If you find yourself trying to replicate
                      % something you found online, ensure you're
                      % loading the necessary packages!
\usepackage{amsfonts} % Math font
\usepackage{fancyvrb}
\usepackage{hyperref} % For including hyperlinks
\usepackage[shortlabels]{enumitem}% For enumerated lists with labels specified
                                  % We had to run tlmgr_install("enumitem") in R
\usepackage{float}    % For telling R where to put a table/figure
\usepackage{natbib}        %For the bibliography
\bibliographystyle{apalike}%For the bibliography

\begin{document}
<<echo=F, message=F, warning=F>>=
library(tidyverse)
@

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item A group of researchers is running an experiment over the course of 30 months, 
with a single observation collected at the end of each month. Let $X_1, ..., X_{30}$
denote the observations for each month. From prior studies, the researchers know that
\[X_i \sim f_X(x),\]
but the mean $\mu_X$ is unknown, and they wish to conduct the following test
\begin{align*}
H_0&: \mu_X = 0\\
H_a&: \mu_X > 0.
\end{align*}
At month $k$, they have accumulated data $X_1, ..., X_k$ and they have the 
$t$-statistic
\[T_k = \frac{\bar{X} - 0}{S_k/\sqrt{n}}.\]
The initial plan was to test the hypotheses after all data was collected (at the 
end of month 30), at level $\alpha=0.05$. However, conducting the experiment is 
expensive, so the researchers want to ``peek" at the data at the end of month 20 
to see if they can stop it early. That is, the researchers propose to check 
whether $t_{20}$ provides statistically discernible support for the alternative. 
If it does, they will stop the experiment early and report support for the 
researcher's alternative hypothesis. If it does not, they will continue to month 
30 and test whether $t_{30}$ provides statistically discernible support for the
alternative.

\begin{enumerate}
  \item What values of $t_{20}$ provide statistically discernible support for the
  alternative hypothesis? \\
Because we are doing a one-sided t-test with a significance level of 0.05, we need to compare $T_{20}$ to the critical value with $n-1=20-1=19$ degrees of freedom, denoted as $t_{0.95, 19}$. This is the value in which 95\% of the t-distribution will lie to the left of it, and 5\% will lie to the right of it. 
<<message=F, warning=F>>=
(crit.value.20 = qt(0.95, df=19))
@
So we got that $t_{0.95, 19} = 1.729$, and because the alternative hypothesis is that the mean is greater than 0, we want the $T_{20}$ values that are greater than 1.729. So there is statistically discernible support for the alternative hypothesis if $T_{20} > 1.729.$
  \item What values of $t_{30}$ provide statistically discernible support for the
  alternative hypothesis? \\
Here we are going to do the same as before, but now with $n-1=30-1=29$ degrees of freedom, denoted as $t_{0.95, 29}$. This again is the value in which 95\% of the t-distribution will lie to the left of it, and 5\% will lie to the right of it.
<<message=F, warning=F>>= 
(crit.value.30 = qt(0.95, df=29))
@
So we got that $t_{0.95, 29} = 1.699$, and because the alternative hypothesis is that the mean is greater than 0, we want the $T_{30}$ values that are greater than 1.699. So there is statistically discernible support for the alternative hypothesis if $T_{30} > 1.699.$
  \item Suppose $f_X(x)$ is a Laplace distribution with $a=0$ and $b=4.0$.
  Conduct a simulation study to assess the Type I error rate of this approach.\\
  \textbf{Note:} You can use the \texttt{rlaplace()} function from the \texttt{VGAM}
  package for \texttt{R} \citep{VGAM}. \\
To simulate the study, I am doing 10000 simulations, where each simulation creates 30 observations of the Laplace(0, 4) distribution. First, I calculate the test statistic for the first 20 observations, and reject if it meets the criteria from part a. If not, then I calculate the test statistic for all 30 observations, and reject if it meets the criteria from part b.
<<message=F, warning=F>>=
library(VGAM)
set.seed(7272)
simulations = 10000 #number of simulations
n = 30 #total sample size
n.peek = 20 #sample size for peeking 
rejections = 0 #counter for number of rejections (rejecting the null)
for(i in 1:simulations){
  x = rlaplace(n, location = 0, scale = 4) #30 observations of Laplace(0, 4) distribution
  x.peek = x[1:20] #first 20 observations
  t.peek = mean(x.peek)/(sd(x.peek)/sqrt(n.peek)) #t statistic after peeking
  if(t.peek > crit.value.20){
    rejections = rejections + 1 #reject if T20 is greater than the critical value with df=19
  }
  else {
    t.final = mean(x)/(sd(x)/sqrt(n)) #t statistic for all 30 observations
    if(t.final > crit.value.30){
      rejections = rejections + 1 #reject if T30 is greater than the critical value with df=29
    }
  }
}
#every rejection is a type 1 error because we are assuming the null is true
(type1.error.rate = rejections / simulations)
@
The Type I error rate is represented by the number of rejections divided by the number of simulations. The reason that each rejection is an occurrence of a Type I error is because we are under the assumption that the null hypothesis is true ($\mu_X = 0$). So based on the seed that I set, I got a Type I error rate of 0.0763, which is greater than our significance level of 0.05. This means that this study increases the chance of getting a Type I error. This is due to the fact that we are checking for rejection twice. If we only checked after 30 observations, then the Type I error rate should be around 0.05, but the probability of rejection goes up when you are testing for it twice per simulation.
  \item \textbf{Optional Challenge:} Can you find a value of $\alpha<0.05$ that yields a 
  Type I error rate of 0.05? \\
Yes, if you change $\alpha$ to 0.033, which changes the critical values to $t_{0.967, 19} = 1.951$, and $t_{0.967, 29} = 1.911$. Because both critical values have increased, it will be less likely for the test statistic to be greater than the critical value, which then decreases the Type I error rate. This significance level of 0.03 yields a Type I error rate of exactly 0.05 given the seed that I set (7272).
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Question 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \item Perform a simulation study to assess the robustness of the $T$ test. 
  Specifically, generate samples of size $n=15$ from the Beta(10,2), Beta(2,10), 
  and Beta(10,10) distributions and conduct the following hypothesis tests against 
  the actual mean for each case (e.g., $\frac{10}{10+2}$, $\frac{2}{10+2}$, and 
  $\frac{10}{10+10}$).
<<message=F, warning=F>>=
n = 15 #sample size
sig.level = 0.05 #significance level
#function that gives Type I error rate for each type of t test for a given Beta distribution
beta.t.test = function(true.mean, alpha, beta){
  #counters for rejections for each type of t test 
  rejections.left = 0
  rejections.right = 0
  rejections.two = 0
  set.seed(7272)
  for(i in 1:simulations){
    x = rbeta(n, shape1 = alpha, shape2 = beta)
    #left tailed test for Ha: mu < true.mean
    left.t.test = t.test(x, mu = true.mean, alternative = "less")
    #right tailed test for Ha: mu > true.mean
    right.t.test = t.test(x, mu = true.mean, alternative = "greater")
    #Two tailed test for Ha: mu != true.mean
    twosided.t.test = t.test(x, mu = true.mean, alternative = "two.sided")
    
    if(left.t.test$p.value < sig.level) { #if p value for left-tailed test is less than 0.05
      rejections.left = rejections.left + 1 #reject
    }
    if(right.t.test$p.value < sig.level) { #if p value for right-tailed test is less than 0.05
      rejections.right = rejections.right + 1 #reject
    }
    if(twosided.t.test$p.value < sig.level) { #if p value for two-tailed test is less than 0.05
      rejections.two = rejections.two + 1 #reject
    }
    
  }
  return (c(left.rate = rejections.left/simulations,
            right.rate = rejections.right/simulations,
            twosided.rate = rejections.two/simulations))
}

(beta.10.2 = beta.t.test(10/12, 10, 2)) #.0306, 0.0790, .0596

(beta.2.10 = beta.t.test(2/12, 2, 10)) #.0790, .0306, .0596

(beta.10.10 = beta.t.test(10/20, 10, 10)) #.0490, .0503, .0498
@
  \begin{enumerate}
    \item What proportion of the time do we make an error of Type I for a
    left-tailed test?
For the Beta(10,2) distribution (left-skewed), the Type I error rate for the left-tailed test was 0.0306. For the Beta(2,10) distribution (right-skewed), it was 0.0790. For the Beta(10,10) distribution (symmetric), it was 0.0490. 
    \item What proportion of the time do we make an error of Type I for a
    right-tailed test?
For the Beta(10,2) distribution (left-skewed), the Type I error rate for the right-tailed test was 0.0790. For the Beta(2,10) distribution (right-skewed), it was 0.0306. For the Beta(10,10) distribution (symmetric), it was 0.0503. As you can see, the left-tailed and right-tailed rates are flipped for the left-skewed and right-skewed distributions.
    \item What proportion of the time do we make an error of Type I for a
    two-tailed test?
For the Beta(10,2) distribution (left-skewed), the Type I error rate for the two-tailed test was 0.0596. For the Beta(2,10) distribution (right-skewed), it was also 0.0596. For the Beta(10,10) distribution (symmetric), it was 0.0498. 
    \item How does skewness of the underlying population distribution effect
    Type I error across the test types?
For left-skewed population distributions (Beta(10,2)), there is going to be a higher Type I error rate (\textgreater 0.05) for the right-tailed test. This is because the t-statistic is more likely to be in the right tail, because more of the data is on the right side. The opposite is true for the left-tailed test (\textless 0.05), because the t-statistic is less likely to be in the left tail. Finally, the two-tailed error rate is slightly higher than 0.05 due to the skewness, but not as high as right-tailed because it is looking at both tails.
\\
For right-skewed population distributions (Beta(2,10)), the opposite is true for left-tailed and right-tailed tests. This is because the majority of the data is now on the left side. So left-tailed tests will yield a higher Type I error rate, while right-tailed tests will yield a lower Type I error rate. The two-tailed error rate is the same as it is for left-skewed population distributions.
\\
Lastly, for symmetric population distributions (Beta(10,10)), all of the tests should result in a Type I error rate similar to 0.05. This is because the distribution is symmetric and approximately normal. So the Type I error rate should be similar to alpha.
  \end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\bibliography{bibliography}
\end{document}
